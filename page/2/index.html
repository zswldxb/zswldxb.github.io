<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logo_2.svg"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logo_2.svg"><link rel="mask-icon" href="/images/logo_2.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zswldxb.github.io",root:"/",scheme:"Mist",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"><meta property="og:type" content="website"><meta property="og:title" content="智商为零的小白的博客"><meta property="og:url" content="https://zswldxb.github.io/page/2/index.html"><meta property="og:site_name" content="智商为零的小白的博客"><meta property="og:description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"><meta property="og:locale" content="zh_CN"><meta property="article:author" content="智商为零的小白"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://zswldxb.github.io/page/2/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!0,isPost:!1,lang:"zh-CN"}</script><title>智商为零的小白的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">智商为零的小白的博客</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content index posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000300.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000300.html" class="post-title-link" itemprop="url">动手学深度学习_softmax回归的从零开始实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:03:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:03:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 10:05:34" itemprop="dateModified" datetime="2021-10-11T10:05:34+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000300.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000300.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="softmax回归的从零开始实现"><a class="markdownIt-Anchor" href="#softmax回归的从零开始实现"></a> softmax回归的从零开始实现</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>本文从零开始实现 <code>softmax</code> 回归，尽管其叫做回归，但其实际是用作多分类任务的。</p><h2 id="生成数据集"><a class="markdownIt-Anchor" href="#生成数据集"></a> 生成数据集</h2><p>这里使用 <code>Fashion-MNIST</code> 数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line"><span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="读取数据"><a class="markdownIt-Anchor" href="#读取数据"></a> 读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 由于一次读取一个数据速度太慢, 这里设置 num_workers 来并行读取数据</span></span><br><span class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line"></span><br><span class="line">train_iter = Data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = Data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure><h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="comment"># X.shape = (n, feature)</span></span><br><span class="line">    X_exp = X.exp()  <span class="comment"># 计算 eps</span></span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 沿特征维度求和</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="comment"># net = softmax(X w + b)</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view(<span class="number">-1</span>, num_inputs), W) + b)</span><br></pre></td></tr></table></figure><div class="post-button"><a class="btn" href="/posts/20211101000300.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000200.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000200.html" class="post-title-link" itemprop="url">动手学深度学习_线性回归的简洁实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:02:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:02:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 09:25:44" itemprop="dateModified" datetime="2021-10-11T09:25:44+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000200.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000200.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="线性回归的简洁实现"><a class="markdownIt-Anchor" href="#线性回归的简洁实现"></a> 线性回归的简洁实现</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>相比于<a href="/posts/20211101000100.html">线性回归的从零开始实现</a>，本文将使用 <code>Pytorch</code> 更方便地实现线性回归的训练。</p><h2 id="生成数据集"><a class="markdownIt-Anchor" href="#生成数据集"></a> 生成数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置维度信息</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_outputs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置真实 w 和 b 以及噪声 eps</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, <span class="number">-3.4</span>]).view(<span class="number">2</span>, <span class="number">-1</span>)</span><br><span class="line">true_b = torch.tensor([<span class="number">4.2</span>])</span><br><span class="line">eps = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_examples, num_outputs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 X 和 y</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs)</span><br><span class="line">labels = torch.mm(features, true_w) + true_b + eps</span><br></pre></td></tr></table></figure><h2 id="读取数据"><a class="markdownIt-Anchor" href="#读取数据"></a> 读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据生成方式</span></span><br><span class="line"><span class="comment"># 使用 Pytorch 定义的 Dataset 和 DataLoader 生成数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_array</span>(<span class="params">data_arrays, batch_size, is_train=True</span>):</span></span><br><span class="line">    dataset = Data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> Data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure><h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_inputs, n_outputs</span>):</span></span><br><span class="line">        super(LinearNet, self).__init__()  <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        self.linear = torch.nn.Linear(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward 定义前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置模型</span></span><br><span class="line">net = LinearNet(num_inputs)</span><br></pre></td></tr></table></figure><p><code>Pytorch</code> 有多种方式生成模型：</p><div class="post-button"><a class="btn" href="/posts/20211101000200.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000100.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000100.html" class="post-title-link" itemprop="url">动手学深度学习_线性回归的从零开始实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:01:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:01:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 09:24:56" itemprop="dateModified" datetime="2021-10-11T09:24:56+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000100.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000100.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>3.8k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>3 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="线性回归的从零开始实现"><a class="markdownIt-Anchor" href="#线性回归的从零开始实现"></a> 线性回归的从零开始实现</h1><h2 id="生成数据集"><a class="markdownIt-Anchor" href="#生成数据集"></a> 生成数据集</h2><p>这里假设样本特征 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1000</mn><mo>×</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{X}\in\R^{1000\times 2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72521em;vertical-align:-.0391em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">X</span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mbin mtight">×</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>，样本标签 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">y</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1000</mn><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{y}\in\R^{1000\times1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">y</span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mord mtight">0</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>，假设样本标签和特征之间的关系满足线性回归方程。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">w</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{y}=\boldsymbol{X}\boldsymbol{w}+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">y</span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.76944em;vertical-align:-.08333em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">X</span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.02778em">w</span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">b</span></span></span></span></span></p><p>为了生成数据，这里假设真实权重 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mo stretchy="false">[</mo><mn>2</mn><mo separator="true">,</mo><mo>−</mo><mn>3.4</mn><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{w}=[2,-3.4]^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.02778em">w</span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">−</span><span class="mord">3</span><span class="mord">.</span><span class="mord">4</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span></span> 和偏差 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>=</mo><mn>4.2</mn></mrow><annotation encoding="application/x-tex">b=4.2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span><span class="mord">.</span><span class="mord">2</span></span></span></span>，以及一个随机噪声项 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">ϵ</span></span></span></span> 来生成标签</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">w</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{y}=\boldsymbol{X}\boldsymbol{w}+b+\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.63888em;vertical-align:-.19444em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.03704em">y</span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.76944em;vertical-align:-.08333em"></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.07778em">X</span></span></span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:.02778em">w</span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">ϵ</span></span></span></span></span></p><p>其中噪声项 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">ϵ</span></span></span></span> 服从均值为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span>、标准差为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0.01</mn></mrow><annotation encoding="application/x-tex">0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">1</span></span></span></span> 的正态分布。噪声代表了数据集中无意义的干扰。下面生成数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置维度信息</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_outputs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置真实 w 和 b 以及噪声 eps</span></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, <span class="number">-3.4</span>]).view(<span class="number">2</span>, <span class="number">-1</span>)</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">eps = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_examples, num_outputs))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 X 和 y</span></span><br><span class="line">features = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, num_inputs))</span><br><span class="line">labels = torch.mm(features, true_w) + true_b + eps</span><br></pre></td></tr></table></figure><h2 id="读取数据"><a class="markdownIt-Anchor" href="#读取数据"></a> 读取数据</h2><p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数：它每次返回 <code>batch_size</code>（批量大小）个随机样本的特征和标签。</p><div class="post-button"><a class="btn" href="/posts/20211101000100.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014500.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014500.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.Transformer</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:45:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:45:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 19:43:26" itemprop="dateModified" datetime="2022-03-05T19:43:26+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014500.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014500.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.9k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>6 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnntransformer"><a class="markdownIt-Anchor" href="#pytorchtorchnntransformer"></a> Pytorch.torch.nn.Transformer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;A transformer model. User is able to modify the attributes as needed. The architecture</span></span><br><span class="line"><span class="string">    is based on the paper &quot;Attention Is All You Need&quot;. Ashish Vaswani, Noam Shazeer,</span></span><br><span class="line"><span class="string">    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and</span></span><br><span class="line"><span class="string">    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information</span></span><br><span class="line"><span class="string">    Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)</span></span><br><span class="line"><span class="string">    model with corresponding parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the encoder/decoder inputs (default=512).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (default=8).</span></span><br><span class="line"><span class="string">        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).</span></span><br><span class="line"><span class="string">        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of encoder/decoder intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        custom_encoder: custom encoder (default=None).</span></span><br><span class="line"><span class="string">        custom_decoder: custom decoder (default=None).</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before</span></span><br><span class="line"><span class="string">            other attention and feedforward operations, otherwise after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note: A full example to apply nn.Transformer module for the word language model is available in</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/examples/tree/master/word_language_model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: int = <span class="number">512</span>, nhead: int = <span class="number">8</span>, num_encoder_layers: int = <span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_decoder_layers: int = <span class="number">6</span>, dim_feedforward: int = <span class="number">2048</span>, dropout: float = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,</span></span></span><br><span class="line"><span class="function"><span class="params">                 custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 layer_norm_eps: float = <span class="number">1e-5</span>, batch_first: bool = False, norm_first: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device=None, dtype=None</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.encoder = custom_encoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                    **factory_kwargs)</span><br><span class="line">            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.decoder = custom_decoder</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                    activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                    **factory_kwargs)</span><br><span class="line">            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.nhead = nhead</span><br><span class="line"></span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Take in and process masked source/target sequences.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            src_mask: the additive mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_mask: the additive mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the additive mask for the encoder output (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: :math:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: :math:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: :math:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: :math:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked</span></span><br><span class="line"><span class="string">            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend</span></span><br><span class="line"><span class="string">            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``</span></span><br><span class="line"><span class="string">            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor</span></span><br><span class="line"><span class="string">            is provided, it will be added to the attention weight.</span></span><br><span class="line"><span class="string">            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by</span></span><br><span class="line"><span class="string">            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero</span></span><br><span class="line"><span class="string">            positions will be unchanged. If a BoolTensor is provided, the positions with the</span></span><br><span class="line"><span class="string">            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            Note: Due to the multi-head attention architecture in the transformer model,</span></span><br><span class="line"><span class="string">            the output sequence length of a transformer is same as the input sequence</span></span><br><span class="line"><span class="string">            (i.e. target) length of the decode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            where S is the source sequence length, T is the target sequence length, N is the</span></span><br><span class="line"><span class="string">            batch size, E is the feature number</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Examples:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.batch_first <span class="keyword">and</span> src.size(<span class="number">1</span>) != tgt.size(<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the batch number of src and tgt must be equal&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.batch_first <span class="keyword">and</span> src.size(<span class="number">0</span>) != tgt.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the batch number of src and tgt must be equal&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> src.size(<span class="number">2</span>) != self.d_model <span class="keyword">or</span> tgt.size(<span class="number">2</span>) != self.d_model:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;the feature number of src and tgt must be equal to d_model&quot;</span>)</span><br><span class="line"></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_square_subsequent_mask</span>(<span class="params">sz: int</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Generate a square mask for the sequence. The masked positions are filled with float(&#x27;-inf&#x27;).</span></span><br><span class="line"><span class="string">            Unmasked positions are filled with float(0.0).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> torch.triu(torch.full((sz, sz), float(<span class="string">&#x27;-inf&#x27;</span>)), diagonal=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Initiate parameters in the transformer model.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                xavier_uniform_(p)</span><br></pre></td></tr></table></figure><p>网络层 <code>Transformer</code> 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.Transformer_figure_1.SVG"><br>图 1</center></div><h2 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;torch version :<span class="subst">&#123;torch.__version__&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># torch version :1.10.0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transformer_model = nn.Transformer(nhead=<span class="number">16</span>, num_encoder_layers=<span class="number">12</span>)</span><br><span class="line">src = torch.rand((<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">tgt = torch.rand((<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>))</span><br><span class="line">output = transformer_model(src, tgt)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014400.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014400.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.TransformerDecoder</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:44:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:44:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 19:24:10" itemprop="dateModified" datetime="2022-03-05T19:24:10+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014400.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014400.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>2.2k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnntransformerdecoder"><a class="markdownIt-Anchor" href="#pytorchtorchnntransformerdecoder"></a> Pytorch.torch.nn.TransformerDecoder</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        decoder_layer: an instance of the TransformerDecoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-decoder-layers in the decoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=None</span>):</span></span><br><span class="line">        super(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_key_padding_mask: Optional[Tensor] = None</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>网络层 <code>TransformerDecoder</code> 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerDecoder_figure_1.SVG"><br>图 1</center></div><h2 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;torch version :<span class="subst">&#123;torch.__version__&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># torch version :1.10.0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">decoder_layer = nn.TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">memory = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">tgt = torch.rand(<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">output = transformer_decoder(tgt, memory)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014300.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014300.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.TransformerEncoder</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:43:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:43:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 19:24:26" itemprop="dateModified" datetime="2022-03-05T19:24:26+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014300.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014300.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>1.7k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnntransformerencoder"><a class="markdownIt-Anchor" href="#pytorchtorchnntransformerencoder"></a> Pytorch.torch.nn.TransformerEncoder</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        encoder_layer: an instance of the TransformerEncoderLayer() class (required).</span></span><br><span class="line"><span class="string">        num_layers: the number of sub-encoder-layers in the encoder (required).</span></span><br><span class="line"><span class="string">        norm: the layer normalization component (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=None</span>):</span></span><br><span class="line">        super(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder (required).</span></span><br><span class="line"><span class="string">            mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>网络层 <code>TransformerEncoder</code> 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerEncoder_figure_1.SVG"><br>图 1</center></div><h2 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;torch version :<span class="subst">&#123;torch.__version__&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># torch version :1.10.0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=<span class="number">6</span>)</span><br><span class="line">src = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">output = transformer_encoder(src)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014200.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014200.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.TransformerDecoderLayer</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:42:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:42:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 19:24:34" itemprop="dateModified" datetime="2022-03-05T19:24:34+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014200.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014200.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.4k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnntransformerdecoderlayer"><a class="markdownIt-Anchor" href="#pytorchtorchnntransformerdecoderlayer"></a> Pytorch.torch.nn.TransformerDecoderLayer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoderLayer</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard decoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False``.</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to self attention, multihead</span></span><br><span class="line"><span class="string">            attention and feedforward operations, respectivaly. Otherwise it&#x27;s done after.</span></span><br><span class="line"><span class="string">            Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=F.relu,</span></span></span><br><span class="line"><span class="function"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=False, norm_first=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device=None, dtype=None</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        super(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(activation, str):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        super(TransformerDecoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = tgt</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm3(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">            x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sa_block</span>(<span class="params">self, x: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]</span>) -&gt; Tensor:</span></span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multihead attention block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_mha_block</span>(<span class="params">self, x: Tensor, mem: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]</span>) -&gt; Tensor:</span></span><br><span class="line">        x = self.multihead_attn(x, mem, mem,</span><br><span class="line">                                attn_mask=attn_mask,</span><br><span class="line">                                key_padding_mask=key_padding_mask,</span><br><span class="line">                                need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout3(x)</span><br></pre></td></tr></table></figure><p>网络层 <code>TransformerDecoderLayer</code> 根据 <code>norm_first=False</code> 或 <code>norm_first=True</code> 可分为两种实现, 分别为图1或图2.</p><p>当 <code>norm_first=False</code> 时, 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerDecoderLayer_figure_1.SVG"><br>图 1</center></div><p>当 <code>norm_first=True</code> 时, 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerDecoderLayer_figure_2.SVG"><br>图 2</center></div><h2 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;torch version :<span class="subst">&#123;torch.__version__&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># torch version :1.10.0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">decoder_layer = nn.TransformerDecoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">memory = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">tgt = torch.rand(<span class="number">20</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">output = decoder_layer(tgt, memory)</span><br><span class="line">print(output.shape)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014100.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014100.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.TransformerEncoderLayer</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:41:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:41:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 19:24:20" itemprop="dateModified" datetime="2022-03-05T19:24:20+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014100.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014100.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.3k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>4 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnntransformerencoderlayer"><a class="markdownIt-Anchor" href="#pytorchtorchnntransformerencoderlayer"></a> Pytorch.torch.nn.TransformerEncoderLayer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoderLayer</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network.</span></span><br><span class="line"><span class="string">    This standard encoder layer is based on the paper &quot;Attention Is All You Need&quot;.</span></span><br><span class="line"><span class="string">    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</span></span><br><span class="line"><span class="string">    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in</span></span><br><span class="line"><span class="string">    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement</span></span><br><span class="line"><span class="string">    in a different way during application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: the number of expected features in the input (required).</span></span><br><span class="line"><span class="string">        nhead: the number of heads in the multiheadattention models (required).</span></span><br><span class="line"><span class="string">        dim_feedforward: the dimension of the feedforward network model (default=2048).</span></span><br><span class="line"><span class="string">        dropout: the dropout value (default=0.1).</span></span><br><span class="line"><span class="string">        activation: the activation function of the intermediate layer, can be a string</span></span><br><span class="line"><span class="string">            (&quot;relu&quot; or &quot;gelu&quot;) or a unary callable. Default: relu</span></span><br><span class="line"><span class="string">        layer_norm_eps: the eps value in layer normalization components (default=1e-5).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False``.</span></span><br><span class="line"><span class="string">        norm_first: if ``True``, layer norm is done prior to attention and feedforward</span></span><br><span class="line"><span class="string">            operations, respectivaly. Otherwise it&#x27;s done after. Default: ``False`` (after).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>, activation=F.relu,</span></span></span><br><span class="line"><span class="function"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, batch_first=False, norm_first=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 device=None, dtype=None</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        super(TransformerEncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(activation, str):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        super(TransformerEncoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            src: the sequence to the encoder layer (required).</span></span><br><span class="line"><span class="string">            src_mask: the mask for the src sequence (optional).</span></span><br><span class="line"><span class="string">            src_key_padding_mask: the mask for the src keys per batch (optional).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            see the docs in Transformer class.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = src</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm2(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">            x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sa_block</span>(<span class="params">self, x: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]</span>) -&gt; Tensor:</span></span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br></pre></td></tr></table></figure><p>网络层 <code>TransformerEncoderLayer</code> 根据 <code>norm_first=False</code> 或 <code>norm_first=True</code> 可分为两种实现, 分别为图1或图2.</p><p>当 <code>norm_first=False</code> 时, 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerEncoderLayer_figure_1.SVG"><br>图 1</center></div><p>当 <code>norm_first=True</code> 时, 实现过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.TransformerEncoderLayer_figure_2.SVG"><br>图 2</center></div><h2 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">print(<span class="string">f&#x27;torch version :<span class="subst">&#123;torch.__version__&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># torch version :1.10.0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=<span class="number">512</span>, nhead=<span class="number">8</span>)</span><br><span class="line">src = torch.rand(<span class="number">10</span>, <span class="number">32</span>, <span class="number">512</span>)</span><br><span class="line">output = encoder_layer(src)</span><br><span class="line">print(output.shape)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003014000.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003014000.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.MultiheadAttention</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:40:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:40:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 14:52:52" itemprop="dateModified" datetime="2022-03-05T14:52:52+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003014000.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003014000.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>34k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>31 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnnmultiheadattention"><a class="markdownIt-Anchor" href="#pytorchtorchnnmultiheadattention"></a> Pytorch.torch.nn.MultiheadAttention</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiheadAttention</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Allows the model to jointly attend to information</span></span><br><span class="line"><span class="string">    from different representation subspaces.</span></span><br><span class="line"><span class="string">    See `Attention Is All You Need &lt;https://arxiv.org/abs/1706.03762&gt;`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \text&#123;MultiHead&#125;(Q, K, V) = \text&#123;Concat&#125;(head_1,\dots,head_h)W^O</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`head_i = \text&#123;Attention&#125;(QW_i^Q, KW_i^K, VW_i^V)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        embed_dim: Total dimension of the model.</span></span><br><span class="line"><span class="string">        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split</span></span><br><span class="line"><span class="string">            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).</span></span><br><span class="line"><span class="string">        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).</span></span><br><span class="line"><span class="string">        bias: If specified, adds bias to input / output projection layers. Default: ``True``.</span></span><br><span class="line"><span class="string">        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.</span></span><br><span class="line"><span class="string">        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.</span></span><br><span class="line"><span class="string">            Default: ``False``.</span></span><br><span class="line"><span class="string">        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).</span></span><br><span class="line"><span class="string">        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>]</span><br><span class="line">    bias_k: Optional[torch.Tensor]</span><br><span class="line">    bias_v: Optional[torch.Tensor]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=True, add_bias_kv=False, add_zero_attn=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None</span>) -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        super(MultiheadAttention, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.kdim = kdim <span class="keyword">if</span> kdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.vdim = vdim <span class="keyword">if</span> vdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self._qkv_same_embed_dim = self.kdim == embed_dim <span class="keyword">and</span> self.vdim == embed_dim</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.head_dim = embed_dim // num_heads</span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == self.embed_dim, <span class="string">&quot;embed_dim must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))</span><br><span class="line">            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;q_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;k_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;v_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.in_proj_bias = Parameter(torch.empty(<span class="number">3</span> * embed_dim, **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> add_bias_kv:</span><br><span class="line">            self.bias_k = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">            self.bias_v = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.bias_k = self.bias_v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.add_zero_attn = add_zero_attn</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim:</span><br><span class="line">            xavier_uniform_(self.in_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xavier_uniform_(self.q_proj_weight)</span><br><span class="line">            xavier_uniform_(self.k_proj_weight)</span><br><span class="line">            xavier_uniform_(self.v_proj_weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.in_proj_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            constant_(self.in_proj_bias, <span class="number">0.</span>)</span><br><span class="line">            constant_(self.out_proj.bias, <span class="number">0.</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_k)</span><br><span class="line">        <span class="keyword">if</span> self.bias_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setstate__</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="comment"># Support loading old MultiheadAttention checkpoints generated by v1.1.0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;_qkv_same_embed_dim&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;_qkv_same_embed_dim&#x27;</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        super(MultiheadAttention, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                need_weights: bool = True, attn_mask: Optional[Tensor] = None</span>) -&gt; Tuple[Tensor, Optional[Tensor]]:</span></span><br><span class="line">        <span class="string">r&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query: Query embeddings of shape :math:`(L, N, E_q)` when ``batch_first=False`` or :math:`(N, L, E_q)`</span></span><br><span class="line"><span class="string">            when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is the batch size,</span></span><br><span class="line"><span class="string">            and :math:`E_q` is the query embedding dimension ``embed_dim``. Queries are compared against</span></span><br><span class="line"><span class="string">            key-value pairs to produce the output. See &quot;Attention Is All You Need&quot; for more details.</span></span><br><span class="line"><span class="string">        key: Key embeddings of shape :math:`(S, N, E_k)` when ``batch_first=False`` or :math:`(N, S, E_k)` when</span></span><br><span class="line"><span class="string">            ``batch_first=True``, where :math:`S` is the source sequence length, :math:`N` is the batch size, and</span></span><br><span class="line"><span class="string">            :math:`E_k` is the key embedding dimension ``kdim``. See &quot;Attention Is All You Need&quot; for more details.</span></span><br><span class="line"><span class="string">        value: Value embeddings of shape :math:`(S, N, E_v)` when ``batch_first=False`` or :math:`(N, S, E_v)` when</span></span><br><span class="line"><span class="string">            ``batch_first=True``, where :math:`S` is the source sequence length, :math:`N` is the batch size, and</span></span><br><span class="line"><span class="string">            :math:`E_v` is the value embedding dimension ``vdim``. See &quot;Attention Is All You Need&quot; for more details.</span></span><br><span class="line"><span class="string">        key_padding_mask: If specified, a mask of shape :math:`(N, S)` indicating which elements within ``key``</span></span><br><span class="line"><span class="string">            to ignore for the purpose of attention (i.e. treat as &quot;padding&quot;). Binary and byte masks are supported.</span></span><br><span class="line"><span class="string">            For a binary mask, a ``True`` value indicates that the corresponding ``key`` value will be ignored for</span></span><br><span class="line"><span class="string">            the purpose of attention. For a byte mask, a non-zero value indicates that the corresponding ``key``</span></span><br><span class="line"><span class="string">            value will be ignored.</span></span><br><span class="line"><span class="string">        need_weights: If specified, returns ``attn_output_weights`` in addition to ``attn_outputs``.</span></span><br><span class="line"><span class="string">            Default: ``True``.</span></span><br><span class="line"><span class="string">        attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape</span></span><br><span class="line"><span class="string">            :math:`(L, S)` or :math:`(N\cdot\text&#123;num\_heads&#125;, L, S)`, where :math:`N` is the batch size,</span></span><br><span class="line"><span class="string">            :math:`L` is the target sequence length, and :math:`S` is the source sequence length. A 2D mask will be</span></span><br><span class="line"><span class="string">            broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.</span></span><br><span class="line"><span class="string">            Binary, byte, and float masks are supported. For a binary mask, a ``True`` value indicates that the</span></span><br><span class="line"><span class="string">            corresponding position is not allowed to attend. For a byte mask, a non-zero value indicates that the</span></span><br><span class="line"><span class="string">            corresponding position is not allowed to attend. For a float mask, the mask values will be added to</span></span><br><span class="line"><span class="string">            the attention weight.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        - **attn_output** - Attention outputs of shape :math:`(L, N, E)` when ``batch_first=False`` or</span></span><br><span class="line"><span class="string">          :math:`(N, L, E)` when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is</span></span><br><span class="line"><span class="string">          the batch size, and :math:`E` is the embedding dimension ``embed_dim``.</span></span><br><span class="line"><span class="string">        - **attn_output_weights** - Attention output weights of shape :math:`(N, L, S)`, where :math:`N` is the batch</span></span><br><span class="line"><span class="string">          size, :math:`L` is the target sequence length, and :math:`S` is the source sequence length. Only returned</span></span><br><span class="line"><span class="string">          when ``need_weights=True``.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            query, key, value = [x.transpose(<span class="number">1</span>, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (query, key, value)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._qkv_same_embed_dim:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask, use_separate_proj_weight=<span class="literal">True</span>,</span><br><span class="line">                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,</span><br><span class="line">                v_proj_weight=self.v_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">return</span> attn_output.transpose(<span class="number">1</span>, <span class="number">0</span>), attn_output_weights</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> attn_output, attn_output_weights</span><br></pre></td></tr></table></figure><p>网络层 <code>MultiheadAttention</code> 为多头注意力机制. 其过程如下:</p><div><center><img src="/images/Pytorch/Pytorch.torch.nn.MultiheadAttention_figure_1.SVG"><br>图 1</center></div><p><code>Pytorch</code> 的官方实现对多种情况都进行了处理, 这里首先介绍 <code>_in_projection()</code>, <code>_in_projection_packed()</code> 函数, 这两个函数实现对 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q,K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.22222em">V</span></span></span></span> 的仿射变换, 然后介绍 <code>_scaled_dot_product_attention()</code> 函数, 该函数实现注意力机制, 最后介绍 <code>multi_head_attention_forward()</code> 函数, 该函数实现了多头注意力机制.</p><h2 id="_in_projection-函数"><a class="markdownIt-Anchor" href="#_in_projection-函数"></a> _in_projection() 函数</h2><ul><li><code>q</code>: <code>query</code> 张量.</li><li><code>k</code>: <code>key</code> 张量.</li><li><code>v</code>: <code>value</code> 张量.</li><li><code>w_q</code>: 对 <code>q</code> 的仿射变换权重张量.</li><li><code>w_k</code>: 对 <code>k</code> 的仿射变换权重张量.</li><li><code>w_v</code>: 对 <code>v</code> 的仿射变换权重张量.</li><li><code>b_q</code>: 对 <code>q</code> 仿射变换偏置张量.</li><li><code>b_k</code>: 对 <code>k</code> 仿射变换偏置张量.</li><li><code>b_v</code>: 对 <code>v</code> 仿射变换偏置张量.</li></ul><p><code>_in_projection()</code> 函数实现 <code>q, k, v</code> 的仿射变换(注意在 <code>multi_head_attention_forward</code> 中进行了维度变换, 这一细节在 <code>multi_head_attention_forward</code> 部分介绍, 本小节中使用的维度数仅在本小节有效, 与其他小节的维度可能名字相同, 但数值可能不同).</p><p>设 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>S</mi><mi>Q</mi></msub><mo>×</mo><msub><mi>N</mi><mi>Q</mi></msub><mo>×</mo><msub><mi>E</mi><mi>Q</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">Q\in\R^{S_{Q}\times N_{Q}\times E_{Q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault">Q</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.10903em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>S</mi><mi>K</mi></msub><mo>×</mo><msub><mi>N</mi><mi>K</mi></msub><mo>×</mo><msub><mi>E</mi><mi>K</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">K\in\R^{S_{K}\times N_{K}\times E_{K}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72243em;vertical-align:-.0391em"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8413309999999999em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.10903em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>S</mi><mi>V</mi></msub><mo>×</mo><msub><mi>N</mi><mi>V</mi></msub><mo>×</mo><msub><mi>E</mi><mi>V</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">V\in\R^{S_{V}\times N_{V}\times E_{V}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72243em;vertical-align:-.0391em"></span><span class="mord mathdefault" style="margin-right:.22222em">V</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8413309999999999em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.10903em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>E</mi><mi>Q</mi></msub><mo>×</mo><msub><mi>E</mi><mi>Q</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^{Q}\in\R^{E_{Q}\times E_{Q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>Q</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>E</mi><mi>Q</mi></msub></msup></mrow><annotation encoding="application/x-tex">B^{Q}\in\R^{E_{Q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>K</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>E</mi><mi>Q</mi></msub><mo>×</mo><msub><mi>E</mi><mi>K</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^{K}\in\R^{E_{Q}\times E_{K}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>K</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>E</mi><mi>Q</mi></msub></msup></mrow><annotation encoding="application/x-tex">B^{K}\in\R^{E_{Q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>V</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>E</mi><mi>Q</mi></msub><mo>×</mo><msub><mi>E</mi><mi>V</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^{V}\in\R^{E_{Q}\times E_{V}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3448em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.14329285714285717em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>B</mi><mi>V</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>E</mi><mi>Q</mi></msub></msup></mrow><annotation encoding="application/x-tex">B^{V}\in\R^{E_{Q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.880431em;vertical-align:-.0391em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.841331em;vertical-align:0"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.841331em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.34479999999999994em"><span style="top:-2.3567071428571427em;margin-left:-.05764em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.28217857142857145em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, 其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q,K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.22222em">V</span></span></span></span> 的维度含义为序列维度, 批量维度和特征维度. 而该函数将 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q,K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.22222em">V</span></span></span></span> 的特征维度数均变为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">E_{Q}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.328331em"><span style="top:-2.5500000000000003em;margin-left:-.05764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span></span></span></span>.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>Q</mi><msup><mrow><mo mathvariant="normal">′</mo></msup></msup><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow><mi>Q</mi><msup><msup><mi>W</mi><mi>Q</mi></msup><mi>T</mi></msup><mo>+</mo><msup><mi>B</mi><mi>Q</mi></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>K</mi><msup><mrow><mo mathvariant="normal">′</mo></msup></msup><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow><mi>K</mi><msup><msup><mi>W</mi><mi>K</mi></msup><mi>T</mi></msup><mo>+</mo><msup><mi>B</mi><mi>K</mi></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>V</mi><msup><mrow><mo mathvariant="normal">′</mo></msup></msup><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow><mi>V</mi><msup><msup><mi>W</mi><mi>V</mi></msup><mi>T</mi></msup><mo>+</mo><msup><mi>B</mi><mi>V</mi></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} Q^{&#x27;}=&amp;Q{W^{Q}}^{T}+B^{Q}\\ K^{&#x27;}=&amp;K{W^{K}}^{T}+B^{K}\\ V^{&#x27;}=&amp;V{W^{V}}^{T}+B^{V}\\ \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:5.3476859999999995em;vertical-align:-2.4238429999999997em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.9238429999999997em"><span style="top:-4.923843em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.99248em"><span style="top:-2.99248em;margin-right:.05em"><span class="pstrut" style="height:2.57948em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8278285714285715em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span></span></span><span style="top:-3.1412809999999998em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.99248em"><span style="top:-2.99248em;margin-right:.05em"><span class="pstrut" style="height:2.57948em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8278285714285715em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span></span></span><span style="top:-1.3587190000000002em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:.22222em">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.99248em"><span style="top:-2.99248em;margin-right:.05em"><span class="pstrut" style="height:2.57948em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8278285714285715em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.4238429999999997em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.9238429999999997em"><span style="top:-4.923843em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"></span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.891331em"><span style="top:-3.1130000000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1225619999999998em"><span style="top:-3.344231em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.891331em"><span style="top:-3.1130000000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">Q</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.1412809999999998em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"></span><span class="mord mathdefault" style="margin-right:.07153em">K</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1225619999999998em"><span style="top:-3.344231em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.07153em">K</span></span></span></span></span></span></span></span></span></span></span><span style="top:-1.3587190000000002em"><span class="pstrut" style="height:3.122562em"></span><span class="mord"><span class="mord"></span><span class="mord mathdefault" style="margin-right:.22222em">V</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1225619999999998em"><span style="top:-3.344231em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:.22222em">V</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.4238429999999997em"><span></span></span></span></span></span></span></span></span></span></span></span></p><div class="post-button"><a class="btn" href="/posts/20211003014000.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211003013900.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211003013900.html" class="post-title-link" itemprop="url">Pytorch.torch.nn.GRU</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-10-03 01:39:00" itemprop="dateCreated datePublished" datetime="2021-10-03T01:39:00+08:00">2021-10-03</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-10 08:40:42" itemprop="dateModified" datetime="2022-03-10T08:40:42+08:00">2022-03-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211003013900.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211003013900.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>22k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>20 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="pytorchtorchnngru"><a class="markdownIt-Anchor" href="#pytorchtorchnngru"></a> Pytorch.torch.nn.GRU</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRU</span>(<span class="params">RNNBase</span>):</span></span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For each element in the input sequence, each layer computes the following</span></span><br><span class="line"><span class="string">    function:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. math::</span></span><br><span class="line"><span class="string">        \begin&#123;array&#125;&#123;ll&#125;</span></span><br><span class="line"><span class="string">            r_t = \sigma(W_&#123;ir&#125; x_t + b_&#123;ir&#125; + W_&#123;hr&#125; h_&#123;(t-1)&#125; + b_&#123;hr&#125;) \\</span></span><br><span class="line"><span class="string">            z_t = \sigma(W_&#123;iz&#125; x_t + b_&#123;iz&#125; + W_&#123;hz&#125; h_&#123;(t-1)&#125; + b_&#123;hz&#125;) \\</span></span><br><span class="line"><span class="string">            n_t = \tanh(W_&#123;in&#125; x_t + b_&#123;in&#125; + r_t * (W_&#123;hn&#125; h_&#123;(t-1)&#125;+ b_&#123;hn&#125;)) \\</span></span><br><span class="line"><span class="string">            h_t = (1 - z_t) * n_t + z_t * h_&#123;(t-1)&#125;</span></span><br><span class="line"><span class="string">        \end&#123;array&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the input</span></span><br><span class="line"><span class="string">    at time `t`, :math:`h_&#123;(t-1)&#125;` is the hidden state of the layer</span></span><br><span class="line"><span class="string">    at time `t-1` or the initial hidden state at time `0`, and :math:`r_t`,</span></span><br><span class="line"><span class="string">    :math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.</span></span><br><span class="line"><span class="string">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In a multilayer GRU, the input :math:`x^&#123;(l)&#125;_t` of the :math:`l` -th layer</span></span><br><span class="line"><span class="string">    (:math:`l &gt;= 2`) is the hidden state :math:`h^&#123;(l-1)&#125;_t` of the previous layer multiplied by</span></span><br><span class="line"><span class="string">    dropout :math:`\delta^&#123;(l-1)&#125;_t` where each :math:`\delta^&#123;(l-1)&#125;_t` is a Bernoulli random</span></span><br><span class="line"><span class="string">    variable which is :math:`0` with probability :attr:`dropout`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input_size: The number of expected features in the input `x`</span></span><br><span class="line"><span class="string">        hidden_size: The number of features in the hidden state `h`</span></span><br><span class="line"><span class="string">        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``</span></span><br><span class="line"><span class="string">            would mean stacking two GRUs together to form a `stacked GRU`,</span></span><br><span class="line"><span class="string">            with the second GRU taking in outputs of the first GRU and</span></span><br><span class="line"><span class="string">            computing the final results. Default: 1</span></span><br><span class="line"><span class="string">        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.</span></span><br><span class="line"><span class="string">            Default: ``True``</span></span><br><span class="line"><span class="string">        batch_first: If ``True``, then the input and output tensors are provided</span></span><br><span class="line"><span class="string">            as `(batch, seq, feature)` instead of `(seq, batch, feature)`.</span></span><br><span class="line"><span class="string">            Note that this does not apply to hidden or cell states. See the</span></span><br><span class="line"><span class="string">            Inputs/Outputs sections below for details.  Default: ``False``</span></span><br><span class="line"><span class="string">        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each</span></span><br><span class="line"><span class="string">            GRU layer except the last layer, with dropout probability equal to</span></span><br><span class="line"><span class="string">            :attr:`dropout`. Default: 0</span></span><br><span class="line"><span class="string">        bidirectional: If ``True``, becomes a bidirectional GRU. Default: ``False``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs: input, h_0</span></span><br><span class="line"><span class="string">        * **input**: tensor of shape :math:`(L, N, H_&#123;in&#125;)` when ``batch_first=False`` or</span></span><br><span class="line"><span class="string">          :math:`(N, L, H_&#123;in&#125;)` when ``batch_first=True`` containing the features of</span></span><br><span class="line"><span class="string">          the input sequence.  The input can also be a packed variable length sequence.</span></span><br><span class="line"><span class="string">          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or</span></span><br><span class="line"><span class="string">          :func:`torch.nn.utils.rnn.pack_sequence` for details.</span></span><br><span class="line"><span class="string">        * **h_0**: tensor of shape :math:`(D * \text&#123;num\_layers&#125;, N, H_&#123;out&#125;)` containing the initial hidden</span></span><br><span class="line"><span class="string">          state for each element in the batch. Defaults to zeros if not provided.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        where:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        .. math::</span></span><br><span class="line"><span class="string">            \begin&#123;aligned&#125;</span></span><br><span class="line"><span class="string">                N =&#123;&#125; &amp; \text&#123;batch size&#125; \\</span></span><br><span class="line"><span class="string">                L =&#123;&#125; &amp; \text&#123;sequence length&#125; \\</span></span><br><span class="line"><span class="string">                D =&#123;&#125; &amp; 2 \text&#123; if bidirectional=True otherwise &#125; 1 \\</span></span><br><span class="line"><span class="string">                H_&#123;in&#125; =&#123;&#125; &amp; \text&#123;input\_size&#125; \\</span></span><br><span class="line"><span class="string">                H_&#123;out&#125; =&#123;&#125; &amp; \text&#123;hidden\_size&#125;</span></span><br><span class="line"><span class="string">            \end&#123;aligned&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs: output, h_n</span></span><br><span class="line"><span class="string">        * **output**: tensor of shape :math:`(L, N, D * H_&#123;out&#125;)` when ``batch_first=False`` or</span></span><br><span class="line"><span class="string">          :math:`(N, L, D * H_&#123;out&#125;)` when ``batch_first=True`` containing the output features</span></span><br><span class="line"><span class="string">          `(h_t)` from the last layer of the GRU, for each `t`. If a</span></span><br><span class="line"><span class="string">          :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output</span></span><br><span class="line"><span class="string">          will also be a packed sequence.</span></span><br><span class="line"><span class="string">        * **h_n**: tensor of shape :math:`(D * \text&#123;num\_layers&#125;, N, H_&#123;out&#125;)` containing the final hidden state</span></span><br><span class="line"><span class="string">          for each element in the batch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        weight_ih_l[k] : the learnable input-hidden weights of the :math:`\text&#123;k&#125;^&#123;th&#125;` layer</span></span><br><span class="line"><span class="string">            (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)` for `k = 0`.</span></span><br><span class="line"><span class="string">            Otherwise, the shape is `(3*hidden_size, num_directions * hidden_size)`</span></span><br><span class="line"><span class="string">        weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\text&#123;k&#125;^&#123;th&#125;` layer</span></span><br><span class="line"><span class="string">            (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`</span></span><br><span class="line"><span class="string">        bias_ih_l[k] : the learnable input-hidden bias of the :math:`\text&#123;k&#125;^&#123;th&#125;` layer</span></span><br><span class="line"><span class="string">            (b_ir|b_iz|b_in), of shape `(3*hidden_size)`</span></span><br><span class="line"><span class="string">        bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\text&#123;k&#125;^&#123;th&#125;` layer</span></span><br><span class="line"><span class="string">            (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        All the weights and biases are initialized from :math:`\mathcal&#123;U&#125;(-\sqrt&#123;k&#125;, \sqrt&#123;k&#125;)`</span></span><br><span class="line"><span class="string">        where :math:`k = \frac&#123;1&#125;&#123;\text&#123;hidden\_size&#125;&#125;`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. note::</span></span><br><span class="line"><span class="string">        For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.</span></span><br><span class="line"><span class="string">        Example of splitting the output layers when ``batch_first=False``:</span></span><br><span class="line"><span class="string">        ``output.view(seq_len, batch, num_directions, hidden_size)``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. include:: ../cudnn_persistent_rnn.rst</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; input = torch.randn(5, 3, 10)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; h0 = torch.randn(2, 3, 20)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; output, hn = rnn(input, h0)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;proj_size&#x27;</span> <span class="keyword">in</span> kwargs:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;proj_size argument is only supported for LSTM, not RNN or GRU&quot;</span>)</span><br><span class="line">        super(GRU, self).__init__(<span class="string">&#x27;GRU&#x27;</span>, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @overload  # type: ignore[override]</span></span><br><span class="line"><span class="meta">    @torch._jit_internal._overload_method  # noqa: F811</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input: Tensor, hx: Optional[Tensor] = None</span>) -&gt; Tuple[Tensor, Tensor]:</span>  <span class="comment"># noqa: F811</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @overload</span></span><br><span class="line"><span class="meta">    @torch._jit_internal._overload_method  # noqa: F811</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input: PackedSequence, hx: Optional[Tensor] = None</span>) -&gt; Tuple[PackedSequence, Tensor]:</span>  <span class="comment"># noqa: F811</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input, hx=None</span>):</span>  <span class="comment"># noqa: F811</span></span><br><span class="line">        orig_input = input</span><br><span class="line">        <span class="comment"># xxx: isinstance check needs to be in conditional for TorchScript to compile</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(orig_input, PackedSequence):</span><br><span class="line">            input, batch_sizes, sorted_indices, unsorted_indices = input</span><br><span class="line">            max_batch_size = batch_sizes[<span class="number">0</span>]</span><br><span class="line">            max_batch_size = int(max_batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch_sizes = <span class="literal">None</span></span><br><span class="line">            max_batch_size = input.size(<span class="number">0</span>) <span class="keyword">if</span> self.batch_first <span class="keyword">else</span> input.size(<span class="number">1</span>)</span><br><span class="line">            sorted_indices = <span class="literal">None</span></span><br><span class="line">            unsorted_indices = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> hx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            num_directions = <span class="number">2</span> <span class="keyword">if</span> self.bidirectional <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">            hx = torch.zeros(self.num_layers * num_directions,</span><br><span class="line">                             max_batch_size, self.hidden_size,</span><br><span class="line">                             dtype=input.dtype, device=input.device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Each batch of the hidden state should match the input sequence that</span></span><br><span class="line">            <span class="comment"># the user believes he/she is passing in.</span></span><br><span class="line">            hx = self.permute_hidden(hx, sorted_indices)</span><br><span class="line"></span><br><span class="line">        self.check_forward_args(input, hx, batch_sizes)</span><br><span class="line">        <span class="keyword">if</span> batch_sizes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,</span><br><span class="line">                             self.dropout, self.training, self.bidirectional, self.batch_first)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,</span><br><span class="line">                             self.num_layers, self.dropout, self.training, self.bidirectional)</span><br><span class="line">        output = result[<span class="number">0</span>]</span><br><span class="line">        hidden = result[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># xxx: isinstance check needs to be in conditional for TorchScript to compile</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(orig_input, PackedSequence):</span><br><span class="line">            output_packed = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)</span><br><span class="line">            <span class="keyword">return</span> output_packed, self.permute_hidden(hidden, unsorted_indices)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output, self.permute_hidden(hidden, unsorted_indices)</span><br></pre></td></tr></table></figure><p>网络层 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>GRU</mtext></mrow><annotation encoding="application/x-tex">\text{GRU}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord text"><span class="mord">GRU</span></span></span></span></span> 为门控循环单元网络. 其过程可参考<a href="/posts/20211003013300.html">Pytorch.torch.nn.RNNCell</a>, <a href="/posts/20211003013400.html">Pytorch.torch.nn.LSTMCell</a>, <a href="/posts/20211003013500.html">Pytorch.torch.nn.GRUCell</a>, <a href="/posts/20211003013700.html">Pytorch.torch.nn.RNN</a>, <a href="/posts/20211003013800.html">Pytorch.torch.nn.LSTM</a>.</p><h3 id="input_size-参数"><a class="markdownIt-Anchor" href="#input_size-参数"></a> input_size 参数</h3><p><code>input_size</code> 参数主要指输入单元的特征数.</p><h3 id="hidden_size-参数"><a class="markdownIt-Anchor" href="#hidden_size-参数"></a> hidden_size 参数</h3><p><code>hidden_size</code> 参数主要指隐藏单元的特征数.</p><h3 id="num_layers-参数"><a class="markdownIt-Anchor" href="#num_layers-参数"></a> num_layers 参数</h3><p><code>num_layers</code> 参数主要指定循环神经网络的层数.</p><h3 id="bias-参数"><a class="markdownIt-Anchor" href="#bias-参数"></a> bias 参数</h3><div class="post-button"><a class="btn" href="/posts/20211003013900.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><nav class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/60/">60</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a></nav></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="智商为零的小白" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">智商为零的小白</p><div class="site-description" itemprop="description">生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">593</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">32</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">92</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zswldxb" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zswldxb" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:1979938740@qq.com" title="E-Mail → mailto:1979938740@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">智商为零的小白</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">2.4m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">35:44 小时</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css"><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'rxJydM3QVoypB9apkSU3e9ML-gzGzoHsz',
      appKey     : 'NqXT8ScfaD1udoMiPk3NQ6MF',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script></body></html>