<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_logo_2.svg"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_logo_2.svg"><link rel="mask-icon" href="/images/logo_2.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/themes/blue/pace-theme-minimal.min.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"zswldxb.github.io",root:"/",scheme:"Mist",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"><meta property="og:type" content="website"><meta property="og:title" content="智商为零的小白的博客"><meta property="og:url" content="https://zswldxb.github.io/index.html"><meta property="og:site_name" content="智商为零的小白的博客"><meta property="og:description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"><meta property="og:locale" content="zh_CN"><meta property="article:author" content="智商为零的小白"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://zswldxb.github.io/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!0,isPost:!1,lang:"zh-CN"}</script><title>智商为零的小白的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">智商为零的小白的博客</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content index posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20191231000000.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20191231000000.html" class="post-title-link" itemprop="url">目录</a></h2><div class="post-meta"><i class="fa fa-thumb-tack"></i> <font color="#ef4136">置顶</font> <span class="post-meta-divider">|</span> <span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-12-31 00:00:00" itemprop="dateCreated datePublished" datetime="2019-12-31T00:00:00+08:00">2019-12-31</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2022-03-05 20:06:52" itemprop="dateModified" datetime="2022-03-05T20:06:52+08:00">2022-03-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20191231000000.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20191231000000.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>13k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="目录"><a class="markdownIt-Anchor" href="#目录"></a> 目录</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>更新日期：2021-10-22 08:48:00</p><p>更新日期：2021-10-27 09:39:00</p><h2 id="正文"><a class="markdownIt-Anchor" href="#正文"></a> 正文</h2><h3 id="洛谷"><a class="markdownIt-Anchor" href="#洛谷"></a> 洛谷</h3><table><thead><tr><th>网址</th></tr></thead><tbody><tr><td><a href="/posts/20200101000000.html">洛谷_P1000_超级玛丽游戏</a></td></tr><tr><td><a href="/posts/20200101000100.html">洛谷_P1001_A+B Problem</a></td></tr><tr><td><a href="/posts/20200101000200.html">洛谷_P1002_过河卒</a></td></tr><tr><td><a href="/posts/20200101000300.html">洛谷_P1003_铺地毯</a></td></tr><tr><td><a href="/posts/20200101000400.html">洛谷_P1004_方格取数</a></td></tr><tr><td><a href="/posts/20200101000500.html">洛谷_P1005_矩阵取数游戏</a></td></tr><tr><td><a href="/posts/20200101000600.html">洛谷_P1006_传纸条</a></td></tr><tr><td><a href="/posts/20200101000700.html">洛谷_P1007_独木桥</a></td></tr><tr><td><a href="/posts/20200101000800.html">洛谷_P1008_三连击</a></td></tr><tr><td><a href="/posts/20200101000900.html">洛谷_P1009_阶乘之和</a></td></tr><tr><td><a href="/posts/20200101001000.html">洛谷_P1010_幂次方</a></td></tr><tr><td><a href="/posts/20200101001200.html">洛谷_P1012_拼数</a></td></tr><tr><td><a href="/posts/20200101001500.html">洛谷_P1015_回文数</a></td></tr><tr><td><a href="/posts/20200101002500.html">洛谷_P1025_[NOIP2001 提高组] 数的划分</a></td></tr><tr><td><a href="/posts/20200101002900.html">洛谷_P1029_最大公约数和最小公倍数问题</a></td></tr><tr><td><a href="/posts/20200101003000.html">洛谷_P1030_求先序排列</a></td></tr><tr><td><a href="/posts/20200101003100.html">洛谷_P1031_[NOIP2002 提高组] 均分纸牌</a></td></tr><tr><td><a href="/posts/20200101003500.html">洛谷_P1035_[NOIP2002 普及组] 级数求和</a></td></tr><tr><td><a href="/posts/20200101003600.html">洛谷_P1036_[NOIP2002 普及组] 选数</a></td></tr><tr><td><a href="/posts/20200101004000.html">洛谷_P1040_[NOIP2003 提高组] 加分二叉树</a></td></tr><tr><td><a href="/posts/20200101004200.html">洛谷_P1042_[NOIP2003 普及组] 乒乓球</a></td></tr><tr><td><a href="/posts/20200101004600.html">洛谷_P1046_[NOIP2005 普及组] 陶陶摘苹果</a></td></tr><tr><td><a href="/posts/20200101004700.html">洛谷_P1047_校门外的树</a></td></tr><tr><td><a href="/posts/20200101005500.html">洛谷_P1055_[NOIP2008 普及组] ISBN 号码</a></td></tr><tr><td><a href="/posts/20200101005700.html">洛谷_P1057_[NOIP2008 普及组] 传球游戏</a></td></tr><tr><td><a href="/posts/20200101010000.html">洛谷_P1060_[NOIP2006 普及组] 开心的金明</a></td></tr><tr><td><a href="/posts/20200101010300.html">洛谷_P1063_[NOIP2006 提高组] 能量项链</a></td></tr><tr><td><a href="/posts/20200101010400.html">洛谷_P1064_[NOIP2006 提高组] 金明的预算方案</a></td></tr><tr><td><a href="/posts/20200101012500.html">洛谷_P1085_[NOIP2004 普及组] 不高兴的津津</a></td></tr><tr><td><a href="/posts/20200101012600.html">洛谷_P1086_[NOIP2004 普及组] 花生采摘</a></td></tr><tr><td><a href="/posts/20200101012700.html">洛谷_P1087_[NOIP2004 普及组] FBI 树</a></td></tr><tr><td><a href="/posts/20200101013800.html">洛谷_P1098_[NOIP2007 提高组] 字符串的展开</a></td></tr><tr><td><a href="/posts/20200101015500.html">洛谷_P1115_最大子段和</a></td></tr><tr><td><a href="/posts/20200101021500.html">洛谷_P1135_奇怪的电梯</a></td></tr><tr><td><a href="/posts/20200101022000.html">洛谷_P1140_相似基因</a></td></tr><tr><td><a href="/posts/20200101022100.html">洛谷_P1141_01迷宫</a></td></tr><tr><td><a href="/posts/20200101024000.html">洛谷_P1160_队列安排</a></td></tr><tr><td><a href="/posts/20200101024200.html">洛谷_P1162_填涂颜色</a></td></tr><tr><td><a href="/posts/20200101024400.html">洛谷_P1164_小A点菜</a></td></tr><tr><td><a href="/posts/20200101025700.html">洛谷_P1177_【模板】快速排序</a></td></tr><tr><td><a href="/posts/20200101030200.html">洛谷_P1182_数列分段 Section II</a></td></tr><tr><td><a href="/posts/20200101031200.html">洛谷_P1192_台阶问题</a></td></tr><tr><td><a href="/posts/20200101032000.html">洛谷_P1200_[USACO1.1]你的飞碟在这儿Your Ride Is Here</a></td></tr><tr><td><a href="/posts/20200101033600.html">洛谷_P1216_[USACO1.5][IOI1994]数字三角形 Number Triangles</a></td></tr><tr><td><a href="/posts/20200101033900.html">洛谷_P1219_[USACO1.5]八皇后 Checker Challenge</a></td></tr><tr><td><a href="/posts/20200101034000.html">洛谷_P1220_关路灯</a></td></tr><tr><td><a href="/posts/20200101043300.html">洛谷_P1273_有线电视网</a></td></tr><tr><td><a href="/posts/20200101043900.html">洛谷_P1279_字串距离</a></td></tr><tr><td><a href="/posts/20200101044000.html">洛谷_P1280_尼克的任务</a></td></tr><tr><td><a href="/posts/20200101044200.html">洛谷_P1282_多米诺骨牌</a></td></tr><tr><td><a href="/posts/20200101050500.html">洛谷_P1305_新二叉树</a></td></tr><tr><td><a href="/posts/20200101050800.html">洛谷_P1308_[NOIP2011 普及组] 统计单词数</a></td></tr><tr><td><a href="/posts/20200101055200.html">洛谷_P1352_没有上司的舞会</a></td></tr><tr><td><a href="/posts/20200101061200.html">洛谷_P1372_又是毕业季I</a></td></tr><tr><td><a href="/posts/20200101061300.html">洛谷_P1373_小a和uim之大逃离</a></td></tr><tr><td><a href="/posts/20200101062700.html">洛谷_P1387_最大正方形</a></td></tr><tr><td><a href="/posts/20200101065200.html">洛谷_P1412_经营与开发</a></td></tr><tr><td><a href="/posts/20200101070100.html">洛谷_P1421_小玉买文具</a></td></tr><tr><td><a href="/posts/20200101070200.html">洛谷_P1422_小玉家的电费</a></td></tr><tr><td><a href="/posts/20200101070300.html">洛谷_P1423_小玉在游泳</a></td></tr><tr><td><a href="/posts/20200101070400.html">洛谷_P1424_小鱼的航程(改进版)</a></td></tr><tr><td><a href="/posts/20200101070500.html">洛谷_P1425_小鱼的游泳时间</a></td></tr><tr><td><a href="/posts/20200101070700.html">洛谷_P1427_小鱼的数字游戏</a></td></tr><tr><td><a href="/posts/20200101070800.html">洛谷_P1428_小鱼比可爱</a></td></tr><tr><td><a href="/posts/20200101072300.html">洛谷_P1443_马的遍历</a></td></tr><tr><td><a href="/posts/20200101072900.html">洛谷_P1449_后缀表达式</a></td></tr><tr><td><a href="/posts/20200101073100.html">洛谷_P1451_求细胞数量</a></td></tr><tr><td><a href="/posts/20200101073300.html">洛谷_P1453_城市环路</a></td></tr><tr><td><a href="/posts/20200101082800.html">洛谷_P1508_Likecloud-吃、吃、吃</a></td></tr><tr><td><a href="/posts/20200101085600.html">洛谷_P1536_村村通</a></td></tr><tr><td><a href="/posts/20200101091100.html">洛谷_P1551_亲戚</a></td></tr><tr><td><a href="/posts/20200101092700.html">洛谷_P1567_统计天数</a></td></tr><tr><td><a href="/posts/20200101101300.html">洛谷_P1613_跑路</a></td></tr><tr><td><a href="/posts/20200101101600.html">洛谷_P1616_疯狂的采药</a></td></tr><tr><td><a href="/posts/20200101102200.html">洛谷_P1622_释放囚犯</a></td></tr><tr><td><a href="/posts/20200101114600.html">洛谷_P1706_全排列问题</a></td></tr><tr><td><a href="/posts/20200101121600.html">洛谷_P1736_创意吃鱼法</a></td></tr><tr><td><a href="/posts/20200101121900.html">洛谷_P1739_表达式括号匹配</a></td></tr><tr><td><a href="/posts/20200101130100.html">洛谷_P1781_宇宙总统</a></td></tr><tr><td><a href="/posts/20200101130200.html">洛谷_P1782_旅行商的背包</a></td></tr><tr><td><a href="/posts/20200101132800.html">洛谷_P1808_单词分类</a></td></tr><tr><td><a href="/posts/20200101142500.html">洛谷_P1865_A % B Problem</a></td></tr><tr><td><a href="/posts/20200101150900.html">洛谷_P1909_[NOIP2016 普及组] 买铅笔</a></td></tr><tr><td><a href="/posts/20200101162000.html">洛谷_P1980_[NOIP2013 普及组] 计数问题</a></td></tr><tr><td><a href="/posts/20200101163600.html">洛谷_P1996_约瑟夫问题</a></td></tr><tr><td><a href="/posts/20200101165400.html">洛谷_P2014_[CTSC1997]选课</a></td></tr><tr><td><a href="/posts/20200101165500.html">洛谷_P2015_二叉苹果树</a></td></tr><tr><td><a href="/posts/20200101165600.html">洛谷_P2016_战略游戏</a></td></tr><tr><td><a href="/posts/20200101170900.html">洛谷_P2029_跳舞</a></td></tr><tr><td><a href="/posts/20200101180400.html">洛谷_P2084_进制转换</a></td></tr><tr><td><a href="/posts/20200101183300.html">洛谷_P2113_看球泡妹子</a></td></tr><tr><td><a href="/posts/20200101190100.html">洛谷_P2141_[NOIP2014 普及组] 珠心算测验</a></td></tr><tr><td><a href="/posts/20200101211900.html">洛谷_P2279_[HNOI2003]消防局的设立</a></td></tr><tr><td><a href="/posts/20200101214700.html">洛谷_P2307_迷宫</a></td></tr><tr><td><a href="/posts/20200101224300.html">洛谷_P2363_马农</a></td></tr><tr><td><a href="/posts/20200102022500.html">洛谷_P2585_[ZJOI2006]三色二叉树</a></td></tr><tr><td><a href="/posts/20200102030300.html">洛谷_P2623_物品选取</a></td></tr><tr><td><a href="/posts/20200102051800.html">洛谷_P2758_编辑距离</a></td></tr><tr><td><a href="/posts/20200102153000.html">洛谷_P3370_【模板】字符串哈希</a></td></tr><tr><td><a href="/posts/20200102223900.html">洛谷_P3799_妖梦拼木棒</a></td></tr><tr><td><a href="/posts/20200103011800.html">洛谷_P3958_[NOIP2017 提高组] 奶酪</a></td></tr></tbody></table><h3 id="蓝桥杯"><a class="markdownIt-Anchor" href="#蓝桥杯"></a> 蓝桥杯</h3><h4 id="begin"><a class="markdownIt-Anchor" href="#begin"></a> BEGIN</h4><table><thead><tr><th>网址</th></tr></thead><tbody><tr><td><a href="/posts/20200201000100.html">蓝桥杯_BEGIN_1_A+B问题</a></td></tr><tr><td><a href="/posts/20200201000200.html">蓝桥杯_BEGIN_2_序列求和</a></td></tr><tr><td><a href="/posts/20200201000300.html">蓝桥杯_BEGIN_3_圆的面积</a></td></tr><tr><td><a href="/posts/20200201000400.html">蓝桥杯_BEGIN_4_Fibonacci数列</a></td></tr></tbody></table><div class="post-button"><a class="btn" href="/posts/20191231000000.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20220101000100.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20220101000100.html" class="post-title-link" itemprop="url">论文代码解析_Deep High Resolution Representation Learning for Visual Recognition</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-01-01 00:01:00" itemprop="dateCreated datePublished" datetime="2022-01-01T00:01:00+08:00">2022-01-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-11-05 08:23:42" itemprop="dateModified" datetime="2021-11-05T08:23:42+08:00">2021-11-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E8%AE%BA%E6%96%87%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" itemprop="url" rel="index"><span itemprop="name">论文代码解析</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20220101000100.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20220101000100.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>30k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>27 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="deep-high-resolution-representation-learning-for-visual-recognition-论文代码解析"><a class="markdownIt-Anchor" href="#deep-high-resolution-representation-learning-for-visual-recognition-论文代码解析"></a> Deep High-Resolution Representation Learning for Visual Recognition 论文代码解析</h1><h2 id="preface"><a class="markdownIt-Anchor" href="#preface"></a> Preface</h2><p>本文为 HRNet 代码解析(这里只关注模型训练过程的重要代码, 对于日志等代码不关注).</p><p>论文的理解见 <a href="/posts/20210701001700.html">论文解析</a></p><h2 id="trainpy"><a class="markdownIt-Anchor" href="#trainpy"></a> <a target="_blank" rel="noopener" href="http://train.py">train.py</a></h2><p>将根目录切换到 <code>/hrnet</code>. 在终端中运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/train.py --cfg experiments/mpii/hrnet/w32_256x256_adam_lr1e-3.yaml</span><br></pre></td></tr></table></figure><p>其将执行 <code>tools/train.py</code> 中的 <code>main()</code> 函数.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    args = parse_args() <span class="comment"># 通过命令行读取参数</span></span><br><span class="line">    update_config(cfg, args) <span class="comment"># 根据命令行中的yaml配置参数更新cfg</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 执行 models.pose_hrnet.get_pose_net(cfg, is_train=True)</span></span><br><span class="line">    model = eval(<span class="string">&#x27;models.&#x27;</span> + cfg.MODEL.NAME + <span class="string">&#x27;.get_pose_net&#x27;</span>)(cfg, is_train=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将 model 并行化, 用多个GPU来加速训练</span></span><br><span class="line">    model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    criterion = JointsMSELoss(use_target_weight=cfg.LOSS.USE_TARGET_WEIGHT).cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    train_dataset = eval(<span class="string">&#x27;dataset.&#x27;</span> + cfg.DATASET.DATASET)(cfg, cfg.DATASET.ROOT, cfg.DATASET.TRAIN_SET, <span class="literal">True</span>, transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        normalize,</span><br><span class="line">    ]))</span><br><span class="line">    valid_dataset = eval(<span class="string">&#x27;dataset.&#x27;</span> + cfg.DATASET.DATASET)(cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, <span class="literal">False</span>, transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        normalize,</span><br><span class="line">    ]))</span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg.TRAIN.BATCH_SIZE_PER_GPU * len(cfg.GPUS), shuffle=cfg.TRAIN.SHUFFLE, num_workers=cfg.WORKERS, pin_memory=cfg.PIN_MEMORY)</span><br><span class="line">    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cfg.TEST.BATCH_SIZE_PER_GPU * len(cfg.GPUS), shuffle=<span class="literal">False</span>, num_workers=cfg.WORKERS, pin_memory=cfg.PIN_MEMORY)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义训练的相关参数</span></span><br><span class="line">    best_perf = <span class="number">0.0</span></span><br><span class="line">    best_model = <span class="literal">False</span></span><br><span class="line">    last_epoch = <span class="number">-1</span></span><br><span class="line">    optimizer = get_optimizer(cfg, model) <span class="comment"># 定义优化器, 这里使用 adam</span></span><br><span class="line">    begin_epoch = cfg.TRAIN.BEGIN_EPOCH</span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.optim.lr_scheduler模块提供了一些根据 epoch 训练次数来调整学习率 (learning rate) 的方法</span></span><br><span class="line">    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, cfg.TRAIN.LR_STEP, cfg.TRAIN.LR_FACTOR, last_epoch=last_epoch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(begin_epoch, cfg.TRAIN.END_EPOCH):</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train for one epoch</span></span><br><span class="line">        train(cfg, train_loader, model, criterion, optimizer, epoch, final_output_dir, tb_log_dir, writer_dict)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># evaluate on validation set</span></span><br><span class="line">        <span class="comment"># 返回该 epoch 的指标</span></span><br><span class="line">        perf_indicator = validate(cfg, valid_loader, valid_dataset, model, criterion, final_output_dir, tb_log_dir, writer_dict)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> perf_indicator &gt;= best_perf:</span><br><span class="line">            best_perf = perf_indicator</span><br><span class="line">            best_model = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            best_model = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型</span></span><br><span class="line">    final_model_state_file = os.path.join(final_output_dir, <span class="string">&#x27;final_state.pth&#x27;</span>)</span><br><span class="line">    torch.save(model.module.state_dict(), final_model_state_file)</span><br></pre></td></tr></table></figure><h2 id="dataset"><a class="markdownIt-Anchor" href="#dataset"></a> Dataset</h2><div class="post-button"><a class="btn" href="/posts/20220101000100.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211201000100.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211201000100.html" class="post-title-link" itemprop="url">VSCode_设置背景</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-12-01 00:01:00" itemprop="dateCreated datePublished" datetime="2021-12-01T00:01:00+08:00">2021-12-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-21 18:55:52" itemprop="dateModified" datetime="2021-10-21T18:55:52+08:00">2021-10-21</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/VSCode/" itemprop="url" rel="index"><span itemprop="name">VSCode</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211201000100.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211201000100.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>750 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="设置背景"><a class="markdownIt-Anchor" href="#设置背景"></a> 设置背景</h1><p>首先运行 <code>VSCode</code>，安装 <code>background</code> 插件。</p><p><img src="/images/VSCode/VSCode_%E8%AE%BE%E7%BD%AE%E8%83%8C%E6%99%AF_figure_1.PNG" alt=""></p><p>然后打开设置，在搜索框中输入 <code>background</code>，选择扩展中的 <code>Plugin background</code>，选择在 <code>setting.json</code> 中编辑。</p><p><img src="/images/VSCode/VSCode_%E8%AE%BE%E7%BD%AE%E8%83%8C%E6%99%AF_figure_2.PNG" alt=""></p><p>修改其中代码，并保存后重启 <code>VSCode</code>。</p><p><img src="/images/VSCode/VSCode_%E8%AE%BE%E7%BD%AE%E8%83%8C%E6%99%AF_figure_3.PNG" alt=""></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&quot;background.customImages&quot;: [</span><br><span class="line">    &quot;file:///D:/Image/background/img_1.jpg&quot;,</span><br><span class="line">    &quot;file:///D:/Image/background/img_2.jpg&quot;,</span><br><span class="line">    &quot;file:///‪D:/Image/background/img_3.jpg&quot;,</span><br><span class="line">],</span><br><span class="line">&quot;background.style&quot;: &#123;</span><br><span class="line">    &quot;content&quot;: &quot;&#x27;&#x27;&quot;,</span><br><span class="line">    &quot;pointer-events&quot;: &quot;none&quot;,</span><br><span class="line">    &quot;position&quot;: &quot;absolute&quot;,</span><br><span class="line">    &quot;z-index&quot;: &quot;999999&quot;,</span><br><span class="line">    &quot;background-position&quot;: &quot;center&quot;,</span><br><span class="line">    &quot;background.repeat&quot;: &quot;no-repeat&quot;,</span><br><span class="line">    &quot;opacity&quot;: 0.2 //透明度</span><br><span class="line">&#125;,</span><br><span class="line">&quot;background.useFront&quot;: true,</span><br><span class="line">&quot;background.useDefault&quot;: false,</span><br></pre></td></tr></table></figure><p><img src="/images/VSCode/VSCode_%E8%AE%BE%E7%BD%AE%E8%83%8C%E6%99%AF_figure_4.PNG" alt=""></p></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101001000.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101001000.html" class="post-title-link" itemprop="url">动手学深度学习_房价预测</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:10:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:10:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-12 21:57:54" itemprop="dateModified" datetime="2021-10-12T21:57:54+08:00">2021-10-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101001000.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101001000.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.8k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>4 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="房价预测"><a class="markdownIt-Anchor" href="#房价预测"></a> 房价预测</h1><p>房价预测是 <code>Kaggle</code> 上的比赛，这里将实现房价预测。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line">train_data = pd.read_csv(dataset_path + <span class="string">&#x27;/Kaggle_house/train.csv&#x27;</span>)</span><br><span class="line">test_data = pd.read_csv(dataset_path + <span class="string">&#x27;/Kaggle_house/test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变特征表示</span></span><br><span class="line"><span class="comment"># 将训练样本与测试样本拼接</span></span><br><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:<span class="number">-1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数值数据按照特征进行标准化</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(<span class="keyword">lambda</span> x: (x - x.mean()) / (x.std()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化后, 每个数值特征的均值变为0, 所以可以直接用0来替换缺失值, 这一步很重要不然后续的pd.get_dummies会得到意料之外的结果</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br><span class="line">print(all_features.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练样本, 训练标签, 测试样本</span></span><br><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)</span><br><span class="line">train_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型, 使用线性回归模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_net</span>(<span class="params">feature_num</span>):</span></span><br><span class="line">    net = nn.Linear(feature_num, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算对数均方根误差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_rmse</span>(<span class="params">net, features, labels</span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 将小于1的值设成1，使得取对数时数值更稳定</span></span><br><span class="line">        clipped_preds = torch.max(net(features), torch.tensor(<span class="number">1.0</span>))</span><br><span class="line">        rmse = torch.sqrt(loss(clipped_preds.log(), labels.log()))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semilogy</span>(<span class="params">x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None, legend=None, figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span></span><br><span class="line">    plt.xlabel(x_label)</span><br><span class="line">    plt.ylabel(y_label)</span><br><span class="line">    plt.semilogy(x_vals, y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">        plt.semilogy(x2_vals, y2_vals, linestyle=<span class="string">&#x27;:&#x27;</span>)</span><br><span class="line">        plt.legend(legend)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练与测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size</span>):</span></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 这里使用了Adam优化算法</span></span><br><span class="line">    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay)</span><br><span class="line">    net = net.float()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X.float()), y.float())</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># k 折交叉验证</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span></span><br><span class="line">    <span class="comment"># 返回第i折交叉验证时所需要的训练和验证数据</span></span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">        idx = slice(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat((X_train, X_part), dim=<span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat((y_train, y_part), dim=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size</span>):</span></span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net(X_train.shape[<span class="number">1</span>])</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[<span class="number">-1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;rmse&#x27;</span>, range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), valid_ls, [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>])</span><br><span class="line">        print(<span class="string">&#x27;fold %d, train rmse %f, valid rmse %f&#x27;</span> % (i, train_ls[<span class="number">-1</span>], valid_ls[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)</span></span><br><span class="line"><span class="comment"># print(&#x27;%d-fold validation: avg train rmse %f, avg valid rmse %f&#x27; % (k, train_l, valid_l))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成预测结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size</span>):</span></span><br><span class="line">    net = get_net(train_features.shape[<span class="number">1</span>])</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>, num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    semilogy(range(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;rmse&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;train rmse %f&#x27;</span> % train_ls[<span class="number">-1</span>])</span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, <span class="number">-1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(dataset_path + <span class="string">&#x27;/Kaggle_house/submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000900.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000900.html" class="post-title-link" itemprop="url">动手学深度学习_丢弃法</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:09:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:09:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-12 21:38:50" itemprop="dateModified" datetime="2021-10-12T21:38:50+08:00">2021-10-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000900.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000900.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="丢弃法"><a class="markdownIt-Anchor" href="#丢弃法"></a> 丢弃法</h1><p>对于某个隐藏层的某个单元 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">h</span></span></span></span>，我们设置丢弃概率为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>，则 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">h</span></span></span></span> 将有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> 的概率被丢弃，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">1-p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> 的概率除以 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">1-p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> 被拉伸，设随机变量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathdefault" style="margin-right:.04601em">ξ</span></span></span></span> 为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 的概率分别为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">1-p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>，则采用丢弃法后新的隐藏层单元 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>h</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mi>ξ</mi><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mstyle><mi>h</mi></mrow><annotation encoding="application/x-tex">h^{&#x27;}=\dfrac{\xi}{1-p}h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.94248em;vertical-align:0"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.94248em"><span style="top:-2.94248em;margin-right:.05em"><span class="pstrut" style="height:2.57948em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8278285714285715em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.25188em;vertical-align:-.8804400000000001em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathdefault">p</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.04601em">ξ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8804400000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault">h</span></span></span></span>。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>x</mi><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">E(xi)=1-p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span>，因此 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><msup><mi>h</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>x</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow></mfrac></mstyle><mi>h</mi><mo>=</mo><mi>h</mi></mrow><annotation encoding="application/x-tex">E(h^{&#x27;})=\dfrac{E(xi)}{1-p}h=h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.19248em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.94248em"><span style="top:-2.94248em;margin-right:.05em"><span class="pstrut" style="height:2.57948em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8278285714285715em"><span style="top:-2.931em;margin-right:.07142857142857144em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-.8804400000000001em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathdefault">p</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord mathdefault">i</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.8804400000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">h</span></span></span></span>，即丢弃法不改变输入的期望值。</p><h2 id="从零开始实现-dropout"><a class="markdownIt-Anchor" href="#从零开始实现-dropout"></a> 从零开始实现 Dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> optimizer</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line">trans = transforms.ToTensor()  <span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(dataset_path, trans, batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 dropout</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span>(<span class="params">X, dropout</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:  <span class="comment"># 在本情况中，所有元素都被丢弃。</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:  <span class="comment"># 在本情况中，所有元素都被保留。</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).float()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = torch.arange(<span class="number">16</span>).view(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line">print(dropout(X, <span class="number">0</span>))</span><br><span class="line">print(dropout(X, <span class="number">0.5</span>))</span><br><span class="line">print(dropout(X, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从零开始实现</span></span><br><span class="line">num_inputs, num_hiddens1, num_hiddens2, num_outputs = <span class="number">784</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">10</span></span><br><span class="line">W1 = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br><span class="line">drop_prod1, drop_prod2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X, is_training=True</span>):</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H1 = dropout(H1, drop_prod1)</span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prod2)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> isinstance(net, torch.nn.Module):</span><br><span class="line">            net.eval()</span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            net.train()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&#x27;is_training&#x27;</span> <span class="keyword">in</span> net.__code__.co_varnames):</span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum.item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.5</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(params, lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size, optimizer=None</span>):</span></span><br><span class="line">    <span class="comment"># 使用 Pytorch 定义的 SGD 训练模型</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;train_l_sum / n:<span class="number">.4</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc_sum / n:<span class="number">.4</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train(net, train_iter, test_iter, loss, num_epochs, batch_size, optimizer)</span><br></pre></td></tr></table></figure><h2 id="dropout-的简洁实现"><a class="markdownIt-Anchor" href="#dropout-的简洁实现"></a> Dropout 的简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> optimizer</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line"><span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(dataset_path, trans, batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        net.eval()</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">        net.train()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Pytorch 定义模型</span></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">num_inputs, num_hiddens1, num_hiddens2, num_outputs, drop_prod1, drop_prod2 = <span class="number">784</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">10</span>, <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line">net = torch.nn.Sequential(torch.nn.Flatten(), torch.nn.Linear(num_inputs, num_hiddens1), torch.nn.ReLU(),</span><br><span class="line">                          torch.nn.Dropout(drop_prod1), torch.nn.Linear(num_hiddens1, num_hiddens2), torch.nn.ReLU(),</span><br><span class="line">                          torch.nn.Dropout(drop_prod2), torch.nn.Linear(num_hiddens2, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    torch.nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练和测试过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size, optimizer=None</span>):</span></span><br><span class="line">    <span class="comment"># 使用 Pytorch 定义的 SGD 训练模型</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;train_l_sum / n:<span class="number">.4</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc_sum / n:<span class="number">.4</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train(net, train_iter, test_iter, loss, num_epochs, batch_size, optimizer)</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000800.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000800.html" class="post-title-link" itemprop="url">动手学深度学习_权重衰减</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:08:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:08:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-12 21:02:44" itemprop="dateModified" datetime="2021-10-12T21:02:44+08:00">2021-10-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000800.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000800.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>1.7k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="权重衰减"><a class="markdownIt-Anchor" href="#权重衰减"></a> 权重衰减</h1><p>本文通过采用权重衰减（L2正则化）来缓解过拟合的问题。</p><h2 id="完整代码"><a class="markdownIt-Anchor" href="#完整代码"></a> 完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 y = w3 x^&#123;3&#125; + w2 x^&#123;2&#125; + w1 x + b</span></span><br><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, torch.tensor([<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]).view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>)</span><br><span class="line">labels = torch.mm(poly_features, true_w) + true_b + torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(n_train + n_test, <span class="number">1</span>))</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">train_features, test_features, train_labels, test_labels</span>):</span></span><br><span class="line">    <span class="comment"># 生成训练数据</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss = torch.nn.MSELoss()</span><br><span class="line">    <span class="comment"># 定义优化函数</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 训练和测试损失</span></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">        train_labels, test_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>), test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())  <span class="comment"># 计算本轮的训练损失</span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())  <span class="comment"># 计算本轮的测试损失</span></span><br><span class="line">    print(<span class="string">f&#x27;final epoch:train loss: <span class="subst">&#123;train_ls[<span class="number">-1</span>]:<span class="number">.4</span>f&#125;</span>, test loss: <span class="subst">&#123;test_ls[<span class="number">-1</span>]:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    print(<span class="string">f&#x27;weight: <span class="subst">&#123;net.weight.data&#125;</span>\n bias: <span class="subst">&#123;net.bias.data&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常情况</span></span><br><span class="line">fit(poly_features[:n_train], poly_features[n_train:], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># 欠拟合</span></span><br><span class="line">fit(features[:n_train], features[n_train:], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># 过拟合</span></span><br><span class="line">fit(poly_features[:<span class="number">2</span>], poly_features[n_train:], labels[:<span class="number">2</span>], labels[n_train:])</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000700.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000700.html" class="post-title-link" itemprop="url">动手学深度学习_模型选择、欠拟合和过拟合</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:07:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:07:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 10:55:26" itemprop="dateModified" datetime="2021-10-11T10:55:26+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000700.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000700.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>1.7k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>2 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="模型选择-欠拟合和过拟合"><a class="markdownIt-Anchor" href="#模型选择-欠拟合和过拟合"></a> 模型选择、欠拟合和过拟合</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>模型训练过程中会出现两类情况：</p><ol><li>模型无法得到较低的训练误差，称作欠拟合（underfitting）。</li><li>训练误差远小于测试误差，称作过拟合（overfitting）。</li></ol><h2 id="完整代码"><a class="markdownIt-Anchor" href="#完整代码"></a> 完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 y = w3 x^&#123;3&#125; + w2 x^&#123;2&#125; + w1 x + b</span></span><br><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, torch.tensor([<span class="number">1.2</span>, <span class="number">-3.4</span>, <span class="number">5.6</span>]).view(<span class="number">-1</span>, <span class="number">1</span>), <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line">poly_features = torch.cat((features, torch.pow(features, <span class="number">2</span>), torch.pow(features, <span class="number">3</span>)), <span class="number">1</span>)</span><br><span class="line">labels = torch.mm(poly_features, true_w) + true_b + torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(n_train + n_test, <span class="number">1</span>))</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">train_features, test_features, train_labels, test_labels</span>):</span></span><br><span class="line">    <span class="comment"># 生成训练数据</span></span><br><span class="line">    batch_size = min(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 定义模型</span></span><br><span class="line">    net = torch.nn.Linear(train_features.shape[<span class="number">-1</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss = torch.nn.MSELoss()</span><br><span class="line">    <span class="comment"># 定义优化函数</span></span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 训练和测试损失</span></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y.view(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 清除梯度</span></span><br><span class="line">            l.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">        train_labels, test_labels = train_labels.view(<span class="number">-1</span>, <span class="number">1</span>), test_labels.view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).item())  <span class="comment"># 计算本轮的训练损失</span></span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).item())  <span class="comment"># 计算本轮的测试损失</span></span><br><span class="line">    print(<span class="string">f&#x27;final epoch:train loss: <span class="subst">&#123;train_ls[<span class="number">-1</span>]:<span class="number">.4</span>f&#125;</span>, test loss: <span class="subst">&#123;test_ls[<span class="number">-1</span>]:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    print(<span class="string">f&#x27;weight: <span class="subst">&#123;net.weight.data&#125;</span>\n bias: <span class="subst">&#123;net.bias.data&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正常情况</span></span><br><span class="line">fit(poly_features[:n_train], poly_features[n_train:], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># 欠拟合</span></span><br><span class="line">fit(features[:n_train], features[n_train:], labels[:n_train], labels[n_train:])</span><br><span class="line"><span class="comment"># 过拟合</span></span><br><span class="line">fit(poly_features[:<span class="number">2</span>], poly_features[n_train:], labels[:<span class="number">2</span>], labels[n_train:])</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000600.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000600.html" class="post-title-link" itemprop="url">动手学深度学习_多层感知机的简洁实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:06:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:06:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 10:32:30" itemprop="dateModified" datetime="2021-10-11T10:32:30+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000600.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000600.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.4k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>4 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="多层感知机的简洁实现"><a class="markdownIt-Anchor" href="#多层感知机的简洁实现"></a> 多层感知机的简洁实现</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>相比于<a href="/posts/20211101000500.html">多层感知机的从零开始实现</a>，本文将使用 <code>Pytorch</code> 更方便地实现多层感知机的训练。</p><h2 id="生成数据集并读取数据"><a class="markdownIt-Anchor" href="#生成数据集并读取数据"></a> 生成数据集并读取数据</h2><p>这里使用 <code>Fashion-MNIST</code> 数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line">trans = transforms.ToTensor()  <span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(dataset_path, trans, batch_size)</span><br></pre></td></tr></table></figure><h2 id="定义模型并初始化模型参数"><a class="markdownIt-Anchor" href="#定义模型并初始化模型参数"></a> 定义模型并初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="comment"># 定义维度</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = <span class="number">784</span>, <span class="number">256</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(num_inputs, num_hiddens), nn.ReLU(), nn.Linear(num_hiddens, num_outputs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">m</span>):</span></span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><h2 id="定义损失函数"><a class="markdownIt-Anchor" href="#定义损失函数"></a> 定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><div class="post-button"><a class="btn" href="/posts/20211101000600.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000500.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000500.html" class="post-title-link" itemprop="url">动手学深度学习_多层感知机的从零开始实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:05:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:05:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 10:23:58" itemprop="dateModified" datetime="2021-10-11T10:23:58+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000500.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000500.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>5.6k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>5 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="多层感知机的从零开始实现"><a class="markdownIt-Anchor" href="#多层感知机的从零开始实现"></a> 多层感知机的从零开始实现</h1><h2 id="生成数据集"><a class="markdownIt-Anchor" href="#生成数据集"></a> 生成数据集</h2><p>这里使用 <code>Fashion-MNIST</code> 数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line">trans = transforms.ToTensor()  <span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h2><a class="markdownIt-Anchor" href="#"></a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">dataset_path = <span class="string">&#x27;../dataset&#x27;</span></span><br><span class="line">trans = transforms.ToTensor()  <span class="comment"># 通过ToTensor将图像数据从PIL类型变换成32位浮点数格式并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(dataset_path, trans, batch_size)</span><br></pre></td></tr></table></figure><h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="comment"># 定义维度</span></span><br><span class="line">num_inputs, num_hiddens, num_outputs = <span class="number">784</span>, <span class="number">256</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">W1 = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens))</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs))</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.max(input=X, other=torch.tensor(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    X = X.view(<span class="number">-1</span>, num_inputs)</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure><h2 id="定义损失函数"><a class="markdownIt-Anchor" href="#定义损失函数"></a> 定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><div class="post-button"><a class="btn" href="/posts/20211101000500.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://zswldxb.github.io/posts/20211101000400.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="智商为零的小白"><meta itemprop="description" content="生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="智商为零的小白的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/posts/20211101000400.html" class="post-title-link" itemprop="url">动手学深度学习_softmax回归的简洁实现</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-11-01 00:04:00" itemprop="dateCreated datePublished" datetime="2021-11-01T00:04:00+08:00">2021-11-01</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-10-11 10:05:52" itemprop="dateModified" datetime="2021-10-11T10:05:52+08:00">2021-10-11</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">动手学深度学习</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/posts/20211101000400.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/posts/20211101000400.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>4.1k 字</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>4 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="softmax回归的简洁实现"><a class="markdownIt-Anchor" href="#softmax回归的简洁实现"></a> softmax回归的简洁实现</h1><h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2><p>相比于<a href="/posts/20211101000300.html">softmax回归的从零开始实现</a>，本文将使用 <code>Pytorch</code> 更方便地实现 <code>softmax</code> 回归的训练。</p><h2 id="生成数据集并读取数据"><a class="markdownIt-Anchor" href="#生成数据集并读取数据"></a> 生成数据集并读取数据</h2><p>这里使用 <code>Fashion-MNIST</code> 数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_fashion_mnist</span>(<span class="params">dataset_path, trans, batch_size</span>):</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=dataset_path, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>) <span class="keyword">else</span> <span class="number">4</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></figure><h2 id="定义模型并初始化模型参数"><a class="markdownIt-Anchor" href="#定义模型并初始化模型参数"></a> 定义模型并初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入输出维度</span></span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">net = nn.Sequential(OrderedDict([(<span class="string">&#x27;flatten&#x27;</span>, nn.Flatten()), (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))]))</span><br><span class="line"><span class="comment"># 初始化模型参数</span></span><br><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="定义损失函数"><a class="markdownIt-Anchor" href="#定义损失函数"></a> 定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><div class="post-button"><a class="btn" href="/posts/20211101000400.html#more" rel="contents">阅读全文 &raquo;</a></div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><nav class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/60/">60</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a></nav></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="智商为零的小白" src="/images/avatar.jpg"><p class="site-author-name" itemprop="name">智商为零的小白</p><div class="site-description" itemprop="description">生命并不是你活了多少日子。而是你记住了多少日子。你要使你过的每一天。都值得记忆。</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">593</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">32</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">92</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zswldxb" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zswldxb" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:1979938740@qq.com" title="E-Mail → mailto:1979938740@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">智商为零的小白</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">2.4m 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">35:44 小时</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css"><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'rxJydM3QVoypB9apkSU3e9ML-gzGzoHsz',
      appKey     : 'NqXT8ScfaD1udoMiPk3NQ6MF',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><canvas class="fireworks" style="position:fixed;left:0;top:0;z-index:1;pointer-events:none"></canvas><script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script><script type="text/javascript" src="/js/src/fireworks.js"></script></body></html>